# 浅谈机器学习原理

*More parameters, easier to overfit?*

*(以宝可梦和数码宝贝的classification为例)*

---



#### Pokemon/Digimon Classifier

##### find a function

we want to find a function. Determine a function with unknown parameters(based on domain knowledge)

![image-20230806114844524](浅谈机器学习原理.assets/image-20230806114844524.png)

---

##### Observation

![image-20230806114940029](浅谈机器学习原理.assets/image-20230806114940029.png)

能否根据线条的复杂程度来判断是宝可梦还是数码宝贝呢?(边缘检测)

- ![image-20230806115152405](浅谈机器学习原理.assets/image-20230806115152405.png)

- Function with Unknown Parameters

  ![image-20230806115306555](浅谈机器学习原理.assets/image-20230806115306555.png)

  question: h如何定值呢?
  $$
  h取值的集合:H={1,2,...,10000}
$$
  |H|:number of candidate functions(model "complexity") 
  
  ---

##### Loss of a function(given data)

- Given a dataset: **D**

  ![image-20230806120105352](浅谈机器学习原理.assets/image-20230806120105352.png)

- Loss of a threshold h given dataset D

  ![image-20230806120237554](浅谈机器学习原理.assets/image-20230806120237554.png)

  当然这个Loss的定义也可以用cross-entropy.

- Training Examples

  - (理想状态)if we can collect all Pokemon and Digimon in the universe $$D_{all}$$, we can find the best threshold h^all^
$$
    h^{all}=argmin_hL(h,D_{all})
    $$

  - (现实状态)we only collect some examples $$D_{train}$$ from $$D_{all}$$(IID)
  
    ![image-20230806121033491](浅谈机器学习原理.assets/image-20230806121033491.png)
  
    $$
    h^{train}=argmin_hL(h,D_{Train})
    $$
  
  - ![image-20230806121226049](浅谈机器学习原理.assets/image-20230806121226049.png)
  
    ![image-20230806155813075](浅谈机器学习原理.assets/image-20230806155813075.png)
  
  -  训练集的Loss更低,测试集更高,容易过拟合
  
    ![image-20230806161157574](浅谈机器学习原理.assets/image-20230806161157574.png)
  
- We want L(h^train^,==D~all~==) - L(h^all^,==D~all~==)<=δ

  - What kind of ==D~train~== fulfill it?

    ![image-20230806162245139](浅谈机器学习原理.assets/image-20230806162245139.png)

  - 推导过程:

    ![image-20230806162923153](浅谈机器学习原理.assets/image-20230806162923153.png)

- we want to sample good D~train~.

  ![image-20230806163030186](浅谈机器学习原理.assets/image-20230806163030186.png)

  > **tips:** 
  >
  > - The following discussion is ==model-agnostic==.
  > - In the following discussion, we don't have assumption about ==data distribution==.
  > - In the following discussion, we can use any ==loss function==

####   Probability of Failure

 train set 好的几率和坏的几率

![image-20230806173339783](浅谈机器学习原理.assets/image-20230806173339783.png)

If a D~train~ is bad, at least one h makes |L(h,D~train~)-L(h,D~all~)| > ε

![image-20230806173630369](浅谈机器学习原理.assets/image-20230806173630369.png)

> 用不同的h计算上面的公式,大于ε的则是坏的训练集(短板效应)

将不同的bad training set 集合并集

![image-20230806181733146](浅谈机器学习原理.assets/image-20230806181733146.png)

![image-20230806192716996](浅谈机器学习原理.assets/image-20230806192716996.png)

将不同的training set算出来的Loss取平均

- The range of loss L is [0,1]
- N is the number of examples in D~train~

![image-20230806193013870](浅谈机器学习原理.assets/image-20230806193013870.png)

> 霍夫丁不等式

则:

![image-20230806193119713](浅谈机器学习原理.assets/image-20230806193119713.png)

取到坏的训练集的概率可能是:
$$
P(D_{train}~is~bad) = U_{h∈H}P(D_{train}is~bad~due~to h)
\\<=~|H|*2e^{-2Nε^{2}}
$$

#### How to make P smaller?

Larger N and smaller |H|.

就是说样本容量N越大,预测越精准,泛化误差上界就越趋近于零

|H|越小指的是可供选择的h的数量变少.

下面看一个例子:

- larter N

  ![image-20230806193957248](浅谈机器学习原理.assets/image-20230806193957248.png)

- Smaller  |H|

  ![image-20230806194016482](浅谈机器学习原理.assets/image-20230806194016482.png)

---

##### Example 

lager N

 ![image-20230806195644298](浅谈机器学习原理.assets/image-20230806195644298.png)如果我们希望P小于一个值,那么我们需要多少训练集呢?N

![image-20230806195745419](浅谈机器学习原理.assets/image-20230806195745419.png)



Model Complexity

|H|中可选取的h是连续的该怎么办呢?

- answer1: Everything that happens in a computer is discrete(不连续的 ).(骗小孩的)
- answer2:VC-dimension(not this course)

![image-20230807142654217](浅谈机器学习原理.assets/image-20230807142654217.png)

Why don'twe simply use a very small |H|?

D~train~ is good means ...

![image-20230807180257394](浅谈机器学习原理.assets/image-20230807180257394.png)

想让大的N和小的|H|.通常情况下很难收集到比较大的N,所以我们选择比较小的|H|.但是选的太小的|H|,Loss会变大,可能就没办法让理想和现实接近.理想会崩坏掉.

![image-20230807180601813](浅谈机器学习原理.assets/image-20230807180601813.png)

- larger |H|: 理想与现实的差距比较大,但是算出来的Loss比较小
- smaller |H|: 理想与现实的差距比较小.理想和现实算出来的Loss接近,但是Loss比较大.![image-20230807181010779](浅谈机器学习原理.assets/image-20230807181010779.png)

##### 有什么办法可以让这两点兼得吗?答案是: Deep Learning!

